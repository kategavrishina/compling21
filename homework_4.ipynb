{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00fad453",
   "metadata": {},
   "source": [
    "# Домашнее задание № 4. Языковые модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d056af4",
   "metadata": {},
   "source": [
    "## Задание 1 (8 баллов)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f532a8",
   "metadata": {},
   "source": [
    "В семинаре для генерации мы использовали предположение маркова и считали, что слово зависит только от 1 предыдущего слова. Но ничто нам не мешает попробовать увеличить размер окна и учитывать два или даже три прошлых слова. Для них мы еще сможем собрать достаточно статистик и, логично предположить, что качество сгенерированного текста должно вырасти."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de743d1d",
   "metadata": {},
   "source": [
    "Попробуйте сделать языковую модель, которая будет учитывать два предыдущих слова при генерации текста.\n",
    "Сгенерируйте несколько текстов (3-5) и расчитайте перплексию получившейся модели. \n",
    "Можно использовать данные из семинара или любые другие (сопоставимые или большие по объему). Перплексию рассчитывайте на 10-50 отложенных предложениях (они не должны использоваться при сборе статистик).\n",
    "\n",
    "\n",
    "Подсказки:  \n",
    "    - нужно будет добавить еще один тэг <start>  \n",
    "    - еще одна матрица не нужна, можно по строкам хронить биграмы, а по колонкам униграммы  \n",
    "    - тексты должны быть очень похожи на нормальные (если у вас получается рандомная каша, вы что-то делаете не так). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d078056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6afcef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = open('lenta.txt', encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e21b938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    normalized_text = [word.text.strip(punctuation) for word \\\n",
    "                                                            in razdel_tokenize(text)]\n",
    "    normalized_text = [word.lower() for word in normalized_text if word and len(word) < 20 ]\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57533985",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_news = normalize(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3121c3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a7c0d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_news = Counter(norm_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8b06a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('в', 0.04808907489694771),\n",
       " ('и', 0.0221080111489724),\n",
       " ('на', 0.018883123731146926),\n",
       " ('по', 0.012943380513471676),\n",
       " ('что', 0.011310349590812525),\n",
       " ('с', 0.01057319451795703),\n",
       " ('не', 0.008435444806676101),\n",
       " ('из', 0.005131529052211166),\n",
       " ('о', 0.00499073907433246),\n",
       " ('как', 0.0049900749706632205),\n",
       " ('к', 0.00407161959610543),\n",
       " ('за', 0.0040125143695431435),\n",
       " ('россии', 0.0036751497055696383),\n",
       " ('для', 0.003325831175549828),\n",
       " ('его', 0.003260084912295149),\n",
       " ('он', 0.0031704309169478593),\n",
       " ('от', 0.003066830744546547),\n",
       " ('сообщает', 0.003050228152815567),\n",
       " ('а', 0.0029180715226369697),\n",
       " ('также', 0.002716184007188258)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas_news = Counter({word:c/len(norm_news) for word, c in vocab_news.items()})\n",
    "probas_news.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "283262a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\katri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "faea3707",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "def ngrammer(tokens, n=2):\n",
    "    ngrams = []\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1835028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ae87765",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 76344/76344 [00:29<00:00, 2590.07it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences_news = [['<start>', '<start>'] + normalize(text) + ['<end>'] for text in tqdm(sent_tokenize(news))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "994a7a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_news = sentences_news[-50:]\n",
    "sentences_news = sentences_news[:-50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fc0529e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams_news = Counter()\n",
    "bigrams_news = Counter()\n",
    "trigrams_news = Counter()\n",
    "\n",
    "for sentence in sentences_news:\n",
    "    unigrams_news.update(sentence)\n",
    "    bigrams_news.update(ngrammer(sentence))\n",
    "    trigrams_news.update(ngrammer(sentence, n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "17057397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<start> <start> в', 7961),\n",
       " ('<start> <start> по', 6210),\n",
       " ('<start> <start> как', 3736),\n",
       " ('<start> <start> однако', 1693),\n",
       " ('<start> <start> на', 1642),\n",
       " ('<start> <start> об', 1618),\n",
       " ('<start> об этом', 1578),\n",
       " ('<start> <start> он', 1551),\n",
       " ('<start> по словам', 1549),\n",
       " ('сообщает риа новости', 1324)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams_news.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "43a66956",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, csc_matrix, lil_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6efdd203",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_news = lil_matrix((len(bigrams_news), \n",
    "                   len(unigrams_news)))\n",
    "\n",
    "id2word_news = list(unigrams_news)\n",
    "word2id_news = {word:i for i, word in enumerate(id2word_news)}\n",
    "\n",
    "id2biword_news = list(bigrams_news)\n",
    "biword2id_news = {word:i for i, word in enumerate(id2biword_news)}\n",
    "\n",
    "\n",
    "for ngram in trigrams_news:\n",
    "    word1, word2, word3 = ngram.split()\n",
    "    bigram = word1 + ' ' + word2\n",
    "    matrix_news[biword2id_news[bigram], word2id_news[word3]] =  (trigrams_news[ngram]/\n",
    "                                                                     bigrams_news[bigram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e2fa30f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_news = csr_matrix(matrix_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "23f4c5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(matrix, id2word, word2id, id2biword, biword2id, n=100, start='<start> <start>'):\n",
    "    text = []\n",
    "    current_idx = biword2id[start]\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        chosen = np.random.choice(list(range(matrix.shape[1])),\n",
    "                                  p=matrix[current_idx].toarray()[0])\n",
    "        text.append(id2word[chosen])\n",
    "        \n",
    "        prev_bigram = id2biword[current_idx].split()[1] + ' ' + id2word[chosen]\n",
    "        current_idx = biword2id[prev_bigram]\n",
    "        \n",
    "        if id2word[chosen] == '<end>':\n",
    "            current_idx = biword2id[start]\n",
    "        \n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1e2b2d",
   "metadata": {},
   "source": [
    "##### Сгенерированные тексты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "455f0563",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "это уже не отрицают но и среди тех кому отказала а также закупать американское оружие и оборудование раздувая собственную значимость и на лестничных площадках выбиты окна и двери \n",
      " как сообщили риа новости молодой человек претендовавший на руку интернетовским сми которые входят в правящее афганское движение талибан открыто встало на сторону бандитов \n",
      " индивидуальные разрешения предоставляются на 5 процентов суммы вкладов превышающий 250-кратный размер минимального размера оплаты труда за каждого незаконно работающего иностранца в лес на 50-й километр ярославского шоссе столкнулись пять машин \n",
      " в распространенном в среду государственная дума могла считаться 50 совладельцем сервера и получала возможность в самое\n"
     ]
    }
   ],
   "source": [
    "print(generate(matrix_news, id2word_news, word2id_news, id2biword_news, biword2id_news).replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ce38d17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "руководство футбольного клуба rad \n",
      " по словам сотрудников правоохранительных органов \n",
      " в результате столкновений с частями и подразделениями огв используя фальшивые документы силандии не заканчивается \n",
      " главные претензии относились на орт а затем будет рассмотрен возможно уже в 2000 году планирует выдать 24-25 тысяч государственных жилищных сертификата для обеспечения общественного порядка уделяя особое внимание будет комплексу множество объектов которогорасположены на территории ставропольского края и области и татарии \n",
      " хотя лидерство халонен выявилось в самом центре лондона где скапливаются огромные толпы народа \n",
      " пожар начался в среду \n",
      " между тем фсб россии по чечне не в меньшем а в интернете \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate(matrix_news, id2word_news, word2id_news, id2biword_news, biword2id_news).replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0ad720a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "здание пиццерии уничтожено практически полностью контролируют обстановку в городе за минувшие сутки в развалинах дома \n",
      " в 4 часа утра на два дня а покупать уголь и мазут затраты энергосистемы выросли на рекордную величину в 4,25 миллиона баррелей в день парламентских выборов в госдуму парламент хорватии в справедливости решения международного суда сомневаются \n",
      " диагноз пока не вижу но внимание таким ситуациям сэр кажется вас надо арестовать за неуплату за два неудавшихся теракта с сухогруза кристина потерпевшего аварию 4 сентября 1998 года аппаратом владимира булгака занимавшего в то время как цена билета тут же взломали подменив главную страницу \n",
      " он отметит\n"
     ]
    }
   ],
   "source": [
    "print(generate(matrix_news, id2word_news, word2id_news, id2biword_news, biword2id_news).replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54ab22b",
   "metadata": {},
   "source": [
    "##### Подсчёт перплексии"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3bf804",
   "metadata": {},
   "source": [
    "Для оригинальных текстов из корпуса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "59bbf453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(probas):\n",
    "    p = np.exp(np.sum(probas))\n",
    "    N = len(probas)\n",
    "    \n",
    "    return p**(-1/N) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "891580d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_perplexity(sentences):\n",
    "    all_probs = []\n",
    "    for sent in sentences:\n",
    "        probs = []\n",
    "        for ngram in ngrammer(sent, n=3):\n",
    "            word1, word2, word3 = ngram.split()\n",
    "            bigram = word1 + ' ' + word2\n",
    "\n",
    "            if bigram in bigrams_news and ngram in trigrams_news:\n",
    "                probs.append(np.log(trigrams_news[ngram]/bigrams_news[bigram]))\n",
    "            else:\n",
    "                probs.append(np.log(0.00001))\n",
    "        all_probs.append(probs)\n",
    "    return np.mean([perplexity(p) for p in all_probs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "7ba85723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20004.95447677816"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_perplexity(test_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0a8dd5",
   "metadata": {},
   "source": [
    "## Задание № 2* (2 балла). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b36c44b",
   "metadata": {},
   "source": [
    "Прочитайте главу про языковое моделирование в книге Журафски и Мартина - https://web.stanford.edu/~jurafsky/slp3/3.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9b1bd8",
   "metadata": {},
   "source": [
    "Развернуто (в пределах 1000 знаков) ответьте на вопросы (по-русски):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2cf844",
   "metadata": {},
   "source": [
    "1. Что можно делать с проблемой несловарных слов? В семинаре мы просто использовали какое-то маленькое значение вероятности, а какие есть другие способы?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116ef503",
   "metadata": {},
   "source": [
    "В книге рассказано про ещё два способа, в каждом из которых реализована идея использования тэга UNK на разных стадиях:\n",
    "\n",
    "1. Это можно делать in advance: задать заранее словарь слов, затем на этапе обучения все слова из обучающей выборки, которые туда не входят, заменять на тэг UNK и воспринимать эти тэги наравне с другими обычными словами из выборки.\n",
    "\n",
    "2. Или можно уже на этапе обучения без предварительного словаря заменять слова с низкой частотностью (вручную задать границу N, ниже которой слова считаются неизвестными, или задать максимальное кол-во слов в словаре - топ-N по частотности как бы, а остальные признать неизвестными) на тэг UNK. Воспринимаются они так же наравне с другими словами."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d1c152",
   "metadata": {},
   "source": [
    "2. Что такое сглаживание (smoothing)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ebd62d",
   "metadata": {},
   "source": [
    "В тестовой выборке могут встретиться такие слова, которые были в словаре, но не встречались в обучающей выборке, или которые были в обучающей выборке, но, например, в другом контексте. Этим словам мы не можем присвоить тэг UNK, но присваивать им нулевую вероятность мы тоже не хотим. В таких случаях предлагается забирать немного вероятности от наиболее вероятных слов/контекстов и давать её этим словам/контекстам, которые не встречались в трейне, таким образом распределяя вероятность между переменными (наиболее вероятные всё равно останутся наиболее вероятными, зато этим невиданным переменным не будет проставляться 0)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
